# -*- coding: utf-8 -*-
"""M4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y3rzcpAUOMYvNJOo3tCUjGODAgSfac8t
"""

# requirements.txt
"""
pandas==2.0.3
numpy==1.24.3
scikit-learn==1.3.0
matplotlib==3.7.2
seaborn==0.12.2
owlready2==0.40
nltk==3.8.1
joblib==1.3.2
wordcloud==1.9.2
"""

# main.py
import pandas as pd
import numpy as np
import re
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from owlready2 import *
import nltk
from nltk.corpus import stopwords
import unicodedata

# Download NLTK resources
nltk.download('punkt', quiet=True)

class AmharicTextPreprocessor:
    def __init__(self):
        # Amharic stop words
        self.amharic_stopwords = set([
            'እና', 'ውስጥ', 'አለ', 'እንደ', 'የ', 'ነበር', 'በ', 'ከ', 'ለ', 'ወደ',
            'እስከ', 'አንድ', 'ያለ', 'ባለ', 'ነው', 'ና', 'ማ', 'ም', 'ን', 'ዎ', 'ው', 'ህ'
        ])

        # Amharic short forms mapping
        self.short_forms = {
            'ዶ/ር': 'ዶክተር',
            'እ/ር': 'እግዚአብሔር',
            'ኢ/ያ': 'ኢትዮጵያ',
            'ጠ/ሚ': 'ጠቅላይ ሚኒስትር',
            'ጠ/ሚኒስቴር': 'ጠቅላይ ሚኒስትር',
            'ወ/ሪት': 'ወይዘሪት',
            'ክ/ከተማ': 'ክፍለ ከተማ',
            'ሪ/ም': 'ሪፖርተር መግለጫ'
        }

        # Amharic character normalization mapping
        self.char_normalization = {
            'ሀ': 'ሃ', 'ሐ': 'ሃ', 'ሓ': 'ሃ', 'ኀ': 'ሃ', 'ኻ': 'ሃ',
            'ሑ': 'ሁ', 'ሗ': 'ሁ', 'ኁ': 'ሁ',
            'ፀ': 'ጸ', 'ፃ': 'ጸ', 'ጸ': 'ጸ'
        }

    def normalize_amharic_chars(self, text):
        """Normalize Amharic character variants"""
        for old_char, new_char in self.char_normalization.items():
            text = text.replace(old_char, new_char)
        return text

    def expand_short_forms(self, text):
        """Expand Amharic short forms to full forms"""
        for short_form, full_form in self.short_forms.items():
            text = text.replace(short_form, full_form)
        return text

    def clean_text(self, text):
        """Clean and normalize Amharic text"""
        if not isinstance(text, str):
            return ""

        # Expand short forms
        text = self.expand_short_forms(text)

        # Normalize characters
        text = self.normalize_amharic_chars(text)

        # Remove English characters and numbers
        text = re.sub(r'[a-zA-Z0-9]', '', text)

        # Remove punctuation and special characters
        text = re.sub(r'[^\w\s]', ' ', text)

        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def remove_stopwords(self, text):
        """Remove Amharic stop words"""
        words = text.split()
        filtered_words = [word for word in words if word not in self.amharic_stopwords]
        return ' '.join(filtered_words)

    def preprocess(self, text):
        """Complete preprocessing pipeline"""
        text = self.clean_text(text)
        text = self.remove_stopwords(text)
        return text

class AmharicNewsOntology:
    def __init__(self, ontology_path=None):
        if ontology_path:
            self.onto = get_ontology(ontology_path).load()
        else:
            self.onto = get_ontology("http://www.semanticweb.org/amharic-news-ontology#")

        # Define semantic weights for concepts
        self.semantic_weights = {
            # Politics-related concepts
            'ፖለቲካ': 0.9, 'መንግስት': 0.8, 'መርጃ': 0.8, 'ፓርቲ': 0.7,
            'ሕግ': 0.7, 'ውጭ': 0.6, 'ፖሊሲ': 0.7,

            # Sports-related concepts
            'ስፖርት': 0.9, 'እግር': 0.8, 'ኳስ': 0.8, 'ቡድን': 0.7,
            'ጨዋታ': 0.7, 'ሻምፕዮን': 0.6, 'ውድድር': 0.7,

            # Business-related concepts
            'ቢዝነስ': 0.9, 'ንግድ': 0.8, 'ገበያ': 0.7, 'ባንክ': 0.7,
            'ምጣኔ': 0.8, 'ዋጋ': 0.6, 'ኩባንያ': 0.7,

            # Entertainment-related concepts
            'መዝናኛ': 0.9, 'ፊልም': 0.8, 'ሙዚቃ': 0.8, 'ቲያትር': 0.7,
            'አርቲስት': 0.7, 'አዝናኝ': 0.6, 'ግጥም': 0.6,

            # News-related concepts
            'ዜና': 0.5, 'ሪፖርተር': 0.6, 'መግለጫ': 0.5, 'ተነጋገር': 0.4
        }

        # Category keywords mapping
        self.category_keywords = {
            'ፖለቲካ': ['ፕሬዝዳንት', 'ጠቅላይ', 'ሚኒስትር', 'ፓርቲ', 'መርጃ', 'ሕግ', 'ፖሊሲ', 'መንግስት'],
            'ስፖርት': ['እግር', 'ኳስ', 'ቡድን', 'ጨዋታ', 'ሻምፕዮን', 'ውድድር', 'ተመልካች', 'ሜዳ'],
            'ቢዝነስ': ['ንግድ', 'ገበያ', 'ባንክ', 'ምጣኔ', 'ዋጋ', 'ኩባንያ', 'ንብረት', 'ኢንቨስት'],
            'መዝናኛ': ['ፊልም', 'ሙዚቃ', 'ቲያትር', 'አርቲስት', 'አዝናኝ', 'ግጥም', 'ዘፈን', 'አስተናጋጅ'],
            'ሀገር አቀፍ ዜና': ['አዲስ', 'አበባ', 'ክፍለ', 'ከተማ', 'ክልል', 'ገደል', 'መንገድ', 'ግንባታ'],
            'አለም አቀፍ ዜና': ['ዓለም', 'ዩናይትድ', 'ኔሽን', 'ዓለም', 'አቀፍ', 'ውጭ', 'ሀገር', 'ዲፕሎማሲ']
        }

    def extract_ontological_features(self, text):
        """Extract semantic features based on ontology concepts"""
        features = np.zeros(30)  # 30-dimensional semantic feature vector

        # Check for category-specific keywords
        for i, (category, keywords) in enumerate(self.category_keywords.items()):
            category_score = 0
            for keyword in keywords:
                if keyword in text:
                    weight = self.semantic_weights.get(keyword, 0.5)
                    # Count occurrences with weighting
                    count = text.count(keyword)
                    category_score += count * weight

            # Normalize by text length
            word_count = len(text.split())
            if word_count > 0:
                category_score /= word_count

            features[i] = category_score

        # Additional semantic features
        # Feature 6-9: Presence of key entities
        features[6] = 1.0 if any(entity in text for entity in ['ፕሬዝዳንት', 'ጠቅላይ', 'ሚኒስትር']) else 0.0
        features[7] = 1.0 if any(entity in text for entity in ['እግር', 'ኳስ', 'ቡድን']) else 0.0
        features[8] = 1.0 if any(entity in text for entity in ['ባንክ', 'ኩባንያ', 'ንግድ']) else 0.0
        features[9] = 1.0 if any(entity in text for entity in ['ፊልም', 'ሙዚቃ', 'አርቲስት']) else 0.0

        # Feature 10-15: Semantic density per category
        for i, category in enumerate(self.category_keywords.keys()):
            keywords = self.category_keywords[category]
            matches = sum(1 for keyword in keywords if keyword in text)
            features[10 + i] = matches / len(keywords) if keywords else 0

        # Feature 16-20: Weighted semantic presence
        for i, category in enumerate(self.category_keywords.keys()):
            total_weight = 0
            for keyword in self.category_keywords[category]:
                if keyword in text:
                    total_weight += self.semantic_weights.get(keyword, 0.5)
            features[16 + i] = total_weight

        # Feature 21-25: Normalized semantic scores
        max_possible = sum(self.semantic_weights.values())
        for i, category in enumerate(self.category_keywords.keys()):
            category_weight = sum(self.semantic_weights.get(k, 0.5) for k in self.category_keywords[category])
            features[21 + i] = features[16 + i] / category_weight if category_weight > 0 else 0

        # Feature 26-29: Contextual features
        features[26] = len(text) / 1000  # Normalized text length
        features[27] = len(text.split()) / 100  # Normalized word count
        features[28] = sum(1 for char in text if char in '!?') / 10  # Question/exclamation density
        features[29] = len(set(text.split())) / len(text.split()) if text.split() else 0  # Lexical diversity

        return features

class AmharicNewsClassifier:
    def __init__(self):
        self.preprocessor = AmharicTextPreprocessor()
        self.ontology = AmharicNewsOntology()
        self.vectorizer = TfidfVectorizer(max_features=20000, max_df=0.95, min_df=2)
        self.model = LogisticRegression(
            multi_class='multinomial',
            solver='lbfgs',
            max_iter=1000,
            random_state=42
        )
        self.label_encoder = LabelEncoder()
        self.is_trained = False

        # Class names
        self.class_names_amharic = ['ሀገር አቀፍ ዜና', 'መዝናኛ', 'ስፖርት', 'ቢዝነስ', 'አለም አቀፍ ዜና', 'ፖለቲካ']
        self.class_names_english = ['Local News', 'Entertainment', 'Sports', 'Business', 'International News', 'Politics']

    def load_data(self, file_path):
        """Load and prepare the dataset"""
        print("Loading dataset...")
        df = pd.read_csv(file_path)

        # Basic data validation
        required_columns = ['article', 'category']
        for col in required_columns:
            if col not in df.columns:
                raise ValueError(f"Required column '{col}' not found in dataset")

        # Remove rows with missing values
        df = df.dropna(subset=['article', 'category'])

        print(f"Dataset loaded: {len(df)} samples")
        print("Category distribution:")
        print(df['category'].value_counts())

        return df

    def prepare_features(self, texts, fit_vectorizer=False):
        """Prepare hybrid features (TF-IDF + Ontological)"""
        print("Preprocessing texts...")
        processed_texts = [self.preprocessor.preprocess(text) for text in texts]

        print("Extracting TF-IDF features...")
        if fit_vectorizer:
            tfidf_features = self.vectorizer.fit_transform(processed_texts)
        else:
            tfidf_features = self.vectorizer.transform(processed_texts)

        print("Extracting ontological features...")
        ontological_features = np.array([self.ontology.extract_ontological_features(text) for text in processed_texts])

        # Combine features
        hybrid_features = np.hstack([tfidf_features.toarray(), ontological_features])

        print(f"Feature dimensions - TF-IDF: {tfidf_features.shape[1]}, Ontological: {ontological_features.shape[1]}")
        print(f"Total features: {hybrid_features.shape[1]}")

        return hybrid_features

    def train(self, file_path, test_size=0.2, random_state=42):
        """Train the classification model"""
        # Load data
        df = self.load_data(file_path)

        # Prepare labels
        print("Encoding labels...")
        y = self.label_encoder.fit_transform(df['category'])

        # Prepare features
        X = self.prepare_features(df['article'].values, fit_vectorizer=True)

        # Split data
        print(f"Splitting data (test_size={test_size})...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )

        print(f"Training samples: {X_train.shape[0]}")
        print(f"Testing samples: {X_test.shape[0]}")

        # Train model
        print("Training Logistic Regression model...")
        self.model.fit(X_train, y_train)
        self.is_trained = True

        # Evaluate
        train_accuracy = self.model.score(X_train, y_train)
        test_accuracy = self.model.score(X_test, y_test)

        print(f"Training accuracy: {train_accuracy:.4f}")
        print(f"Testing accuracy: {test_accuracy:.4f}")

        # Detailed evaluation
        y_pred = self.model.predict(X_test)
        self.evaluate_model(y_test, y_pred)

        return X_train, X_test, y_train, y_test, y_pred

    def evaluate_model(self, y_true, y_pred):
        """Comprehensive model evaluation"""
        print("\n" + "="*50)
        print("MODEL EVALUATION")
        print("="*50)

        # Overall accuracy
        accuracy = accuracy_score(y_true, y_pred)
        print(f"Overall Accuracy: {accuracy:.4f}")

        # Classification report
        print("\nClassification Report:")
        print(classification_report(y_true, y_pred,
                                  target_names=self.class_names_english,
                                  digits=4))

        # Confusion matrix
        plt.figure(figsize=(10, 8))
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.class_names_english,
                   yticklabels=self.class_names_english)
        plt.title('Confusion Matrix - Amharic News Classification')
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Per-class accuracy
        print("\nPer-class Accuracy:")
        class_accuracy = cm.diagonal() / cm.sum(axis=1)
        for i, (am, en) in enumerate(zip(self.class_names_amharic, self.class_names_english)):
            print(f"{en} ({am}): {class_accuracy[i]:.4f}")

    def predict(self, text):
        """Predict category for new text"""
        if not self.is_trained:
            raise ValueError("Model must be trained before prediction")

        # Preprocess and extract features
        features = self.prepare_features([text])

        # Predict
        prediction = self.model.predict(features)[0]
        probability = self.model.predict_proba(features)[0]

        # Get results
        predicted_class_am = self.class_names_amharic[prediction]
        predicted_class_en = self.class_names_english[prediction]

        # Get probabilities for all classes
        class_probabilities = {
            f"{en} ({am})": f"{prob:.4f}"
            for am, en, prob in zip(self.class_names_amharic, self.class_names_english, probability)
        }

        return {
            'predicted_class_amharic': predicted_class_am,
            'predicted_class_english': predicted_class_en,
            'confidence': max(probability),
            'all_probabilities': class_probabilities
        }

    def ablation_study(self, file_path):
        """Compare TF-IDF only vs Hybrid approach"""
        print("="*50)
        print("ABLATION STUDY")
        print("="*50)

        df = self.load_data(file_path)
        y = self.label_encoder.fit_transform(df['category'])

        # TF-IDF Only
        print("\n1. TF-IDF Only Approach:")
        processed_texts = [self.preprocessor.preprocess(text) for text in df['article'].values]
        X_tfidf = self.vectorizer.fit_transform(processed_texts)

        X_train, X_test, y_train, y_test = train_test_split(
            X_tfidf, y, test_size=0.2, random_state=42, stratify=y
        )

        tfidf_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)
        tfidf_model.fit(X_train, y_train)
        tfidf_accuracy = tfidf_model.score(X_test, y_test)
        print(f"TF-IDF Only Accuracy: {tfidf_accuracy:.4f}")

        # Hybrid Approach
        print("\n2. Hybrid Approach (TF-IDF + Ontology):")
        X_hybrid = self.prepare_features(df['article'].values, fit_vectorizer=True)

        X_train, X_test, y_train, y_test = train_test_split(
            X_hybrid, y, test_size=0.2, random_state=42, stratify=y
        )

        self.model.fit(X_train, y_train)
        hybrid_accuracy = self.model.score(X_test, y_test)
        print(f"Hybrid Approach Accuracy: {hybrid_accuracy:.4f}")

        improvement = hybrid_accuracy - tfidf_accuracy
        print(f"\nImprovement with Ontology: {improvement:.4f} ({improvement*100:.2f}%)")

        return tfidf_accuracy, hybrid_accuracy

    def save_model(self, filepath):
        """Save the trained model and components"""
        if not self.is_trained:
            raise ValueError("No trained model to save")

        model_data = {
            'model': self.model,
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder,
            'class_names_amharic': self.class_names_amharic,
            'class_names_english': self.class_names_english
        }

        joblib.dump(model_data, filepath)
        print(f"Model saved to {filepath}")

    def load_model(self, filepath):
        """Load a trained model"""
        model_data = joblib.load(filepath)

        self.model = model_data['model']
        self.vectorizer = model_data['vectorizer']
        self.label_encoder = model_data['label_encoder']
        self.class_names_amharic = model_data['class_names_amharic']
        self.class_names_english = model_data['class_names_english']
        self.is_trained = True

        print(f"Model loaded from {filepath}")

# Example usage and testing
def main():
    # Initialize classifier
    classifier = AmharicNewsClassifier()

    # Dataset path (update this path)
    dataset_path = "/content/drive/MyDrive/Amharic News Dataset.csv"

    try:
        # Perform ablation study
        print("PERFORMING ABLATION STUDY...")
        tfidf_acc, hybrid_acc = classifier.ablation_study(dataset_path)

        # Train final model
        print("\n" + "="*50)
        print("TRAINING FINAL MODEL...")
        print("="*50)

        X_train, X_test, y_train, y_test, y_pred = classifier.train(dataset_path)

        # Save model
        classifier.save_model("amharic_news_classifier.pkl")

        # Test predictions
        print("\n" + "="*50)
        print("SAMPLE PREDICTIONS")
        print("="*50)

        test_samples = [
            "የኢትዮጵያ ብሔራዊ እግር ኳስ ቡድን አዲስ አበባ ከገባ በኋላ ለውድድር ማጠናከሪያ ጀመረ",  # Sports
            "የመንግስት አዲስ ፖሊሲ ለንግድ ማህበራት ተጨማሪ ዕድል ያረጋል",  # Business/Politics
            "አዲስ የፊልም ፕሮጀክት በሙዚቃ አርቲስቶች ተለዋዋጭ አስተዋጽኦ ቀርቧል",  # Entertainment
            "የአዲስ አበባ መንገድ ግንባታ ለሚቀጥሉት ሁለት ሳምንታት ይዘገያል"  # Local News
        ]

        for i, sample in enumerate(test_samples, 1):
            print(f"\nSample {i}: {sample}")
            result = classifier.predict(sample)
            print(f"Prediction: {result['predicted_class_english']} ({result['predicted_class_amharic']})")
            print(f"Confidence: {result['confidence']:.4f}")
            print("All probabilities:")
            for class_name, prob in result['all_probabilities'].items():
                print(f"  {class_name}: {prob}")

    except Exception as e:
        print(f"Error: {e}")
        print("Please check the dataset path and format")

if __name__ == "__main__":
    main()

# inference_app.py (Optional - for web interface)
"""
from flask import Flask, request, jsonify, render_template
import joblib

app = Flask(__name__)

# Load model
classifier = AmharicNewsClassifier()
classifier.load_model("amharic_news_classifier.pkl")

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        text = data.get('text', '')

        if not text:
            return jsonify({'error': 'No text provided'}), 400

        result = classifier.predict(text)
        return jsonify(result)

    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True)
"""

# create_sample_dataset.py (If you need to create a sample dataset)
"""
import pandas as pd
import numpy as np

def create_sample_dataset():
    # Sample Amharic news data
    sample_data = {
        'article': [
            # Local News
            "የአዲስ አበባ ከተማ አስተዳደር አዲስ የመንገድ ፕሮጀክት አስተዋወቀ ። ይህ ፕሮጀክት የተራራውን አካባቢ የሚያገናኝ ሲሆን ለሚቀጥሉት ሁለት ዓመታት ይጠናቀቃል ።",

            # Entertainment
            "ታዋቂ ዘፋኝ ቴዲ አፍሮ አዲስ አልበም አሳትሟል ። አልበሙ በሺዎች የሚቆጠሩ ሰዎች በመጀመሪያው ቀን ወደ ሱቆች በመግባት ገዝተዋል ።",

            # Sports
            "የኢትዮጵያ ብሔራዊ እግር ኳስ ቡድን በአፍሪካ ዋንጫ ማጠናከሪያ ውድድር ላይ አሳይጦ አለ ። ቡድኑ ለሚቀጥለው ውድድር በጣም ተስፋ አድርጓል ።",

            # Business
            "የኢትዮጵያ ንግድ ባንክ አዲስ የዲጂታል አገልግሎት አስጀመረ ። ይህ አገልግሎት ለደንበኞች የበለጠ ምቾት ያስገኛል ።",

            # International News
            "የተባበሩት መንግስታት ድርጅት በምስራቅ አፍሪካ ክልል ለሰላም ሂደት አዲስ ተጨማሪ ድጋፍ እንደሚያደርግ አስታወቀ ።",

            # Politics
            "የመንግስት አዲስ የኢኮኖሚ ፖሊሲ በፓርላማ ቀርቦ ነው ። ፖሊሲው የውጭ ኢንቨስትመንት ለማሰባሰብ አዳዲስ ማነቃቂያዎችን ያካትታል ።"
        ],
        'category': [
            'ሀገር አቀፍ ዜና',
            'መዝናኛ',
            'ስፖርት',
            'ቢዝነስ',
            'አለም አቀፍ ዜና',
            'ፖለቲካ'
        ]
    }

    df = pd.DataFrame(sample_data)
    df.to_csv('sample_amharic_news.csv', index=False, encoding='utf-8')
    print("Sample dataset created: sample_amharic_news.csv")

if __name__ == "__main__":
    create_sample_dataset()
"""